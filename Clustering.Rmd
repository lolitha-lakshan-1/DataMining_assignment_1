setwd("/Users/lolitha/Desktop/Assignment/Assignment_1")

library(tidyverse)
library(cluster)
library(factoextra)
library(ggplot2)
library(GGally)
library(mice)

# Load dataset
data <- read.csv("vehicles.csv", header = TRUE)

# Preview dataset
head(data)
dim(data)
names(data)

print(nrow(data))

# 1. Missing values
colSums(is.na(data))

# 2. Summary
summary(data)
print(nrow(data))

# 3. Replace missing values 
class_labels <- data$class
data_features <- data %>% select(-class)
imputed <- mice(data_features, method = "mean", m = 1, maxit = 1, seed = 123)
completed_data <- complete(imputed, 1)
vehicles_clean <- completed_data
vehicles_clean$class <- class_labels

print(nrow(vehicles_clean))

# 4. Display Outliers

summary(data)

numeric_features_with_outliers <- vehicles_clean %>% select(-class)

numeric_long <- numeric_features_with_outliers %>%
  mutate(row_id = row_number()) %>%
  pivot_longer(-row_id, names_to = "Feature", values_to = "Value")
  
ggplot(numeric_long, aes(x = Feature, y = Value)) +
  geom_boxplot(outlier.colour = "red", outlier.alpha = 0.6) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Boxplot of Features (Before Outlier Removal)", y = "Value", x = "Feature")

# find outlier counts
outlier_counts <- sapply(numeric_features_with_outliers, function(column) {
  Q1 <- quantile(column, 0.25, na.rm = TRUE)
  Q3 <- quantile(column, 0.75, na.rm = TRUE)
  IQR_val <- Q3 - Q1
  lower <- Q1 - 1.5 * IQR_val
  upper <- Q3 + 1.5 * IQR_val
  sum(column < lower | column > upper, na.rm = TRUE)
})

outlier_counts_df <- data.frame(Feature = names(outlier_counts),
                                Outlier_Count = outlier_counts)
print(outlier_counts_df)

# Cap Outliers
cap_outliers <- function(df) {
  df_capped <- df
  for (col in names(df_capped)) {
    Q1 <- quantile(df_capped[[col]], 0.25, na.rm = TRUE)
    Q3 <- quantile(df_capped[[col]], 0.75, na.rm = TRUE)
    IQR_val <- Q3 - Q1
    lower <- Q1 - 1.5 * IQR_val
    upper <- Q3 + 1.5 * IQR_val
    
    df_capped[[col]][df_capped[[col]] < lower] <- lower
    df_capped[[col]][df_capped[[col]] > upper] <- upper
  }
  return(df_capped)
}

features_capped <- cap_outliers(numeric_features_with_outliers)

features_no_outliers_long <- features_capped %>%
  mutate(row_id = row_number()) %>%
  pivot_longer(-row_id, names_to = "Feature", values_to = "Value")

# Plot boxplots after capping outliers
ggplot(features_no_outliers_long, aes(x = Feature, y = Value)) +
  geom_boxplot(fill = "skyblue", outlier.shape = NA) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Boxplot of Features (After Outlier Removal)",
       x = "Feature", y = "Value")

# Scaling
# Applies z-score scaling to all numeric features
# Excludes the class column (since it's categorical)
# After this, all features are on the same scale, and now:
# - PCA won't be biased
# - K-Means won’t treat large-scale features as more “important”
features_scaled <- scale(features_capped)
print(features_scaled)

# PCA Dimensionality Reduction
#center = TRUE: subtracts the mean of each feature before PCA.
#scale. = TRUE: divides by standard deviation (optional since scaled, but still ensures standardization).
pca_result <- prcomp(features_scaled, center = TRUE, scale. = TRUE)
summary(pca_result)
print(pca_result$rotation)

# Scree plot
fviz_eig(pca_result, addlabels = TRUE, ylim = c(0, 50))

# Cumulative variance 
cum_var <- cumsum(pca_result$sdev^2) / sum(pca_result$sdev^2)
cum_var_df <- data.frame(PC = 1:length(cum_var), CumulativeVariance = cum_var)

ggplot(cum_var_df, aes(x = PC, y = CumulativeVariance)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = 0.90, linetype = "dashed", color = "red") +
  labs(title = "Cumulative Variance Explained by Principal Components",
       y = "Cumulative Variance", x = "Number of Principal Components") +
  theme_minimal()
  
#The red dashed line is at 0.90 cumulative variance.
#The black curve crosses 0.90 somewhere between PC 4 and PC 5.
#Means:
#PC1–PC4 < 90%
#PC1–PC5 ≥ 90%

# Determine optimal number of clusters using Elbow Method
fviz_nbclust(pca_selected, kmeans, method = "wss")

# Apply K-Means
set.seed(123)
kmeans_result <- kmeans(pca_selected, centers = 4, nstart = 25)

# Add cluster info to PCA data for plotting
pca_data$cluster <- as.factor(kmeans_result$cluster)

# Visualize Clusters in PCA space
ggplot(pca_data, aes(x = PC1, y = PC2, color = cluster, shape = class)) +
  geom_point(alpha = 0.7) +
  labs(title = "K-Means Clustering on PCA-Reduced Data", x = "PC1", y = "PC2") +
  theme_minimal()

# Cluster Profiling
features_capped$cluster <- kmeans_result$cluster
cluster_means <- features_capped %>%
  group_by(cluster) %>%
  summarise(across(where(is.numeric), mean))
print(cluster_means, width = Inf)

# Compare Clusters with Actual Classes
print(table(Cluster = features_capped$cluster, Class = vehicles_clean$class))

############################################## Hierarchical Clustering after PCA (R) ###############################################


# Compute distance
# Euclidean distance reflects how far two points are from each other in terms of overall variance-base and data is continous
distance_matrix <- dist(pca_selected, method = "euclidean")

#Agglomerative Clustering
# Ward method works only with squared Euclidean distances
hc <- hclust(distance_matrix, method = "ward.D2")

#Plot the Dendrogram
plot(hc, labels = FALSE, hang = -1, main = "Hierarchical Clustering Dendrogram")

rect.hclust(hc, k = 4, border = 2:5)

#Cut Dendrogram into k Clusters
cluster_cut <- cutree(hc, k = 4)

# Plot PCA
pca_df <- as.data.frame(pca_selected)
pca_df$cluster <- factor(cluster_cut)
pca_df$class <- vehicles_clean$class

ggplot(pca_df, aes(x = PC1, y = PC2, color = cluster, shape = class)) +
  geom_point(alpha = 0.7) +
  labs(title = "Hierarchical Clustering (Ward) on PCA-Reduced Data",
       x = "PC1", y = "PC2") +
  theme_minimal()
  
# Confusion Matrix
table(Cluster = pca_df$cluster, Class = pca_df$class)

