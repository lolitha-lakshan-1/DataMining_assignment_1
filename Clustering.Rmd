setwd("/Users/lolitha/Desktop/Assignment/Assignment_1")

library(tidyverse)
library(cluster)
library(factoextra)
library(ggplot2)
library(GGally)

# Load dataset
data <- read.csv("vehicles.csv", header = TRUE)

# Preview dataset
head(data)
dim(data)
names(data)

print(nrow(data))

# 1. Missing values
colSums(is.na(data))

# 2. Summary
summary(data)

print(nrow(data))
# 3. Remove missing values
vehicles_clean <- na.omit(data)


print(nrow(vehicles_clean))

# 4. Detect and remove outliers using IQR

#Display Outliers
numeric_features_with_outliers <- vehicles_clean %>% select(-class)

numeric_long <- numeric_features_with_outliers %>%
  mutate(row_id = row_number()) %>%
  pivot_longer(-row_id, names_to = "Feature", values_to = "Value")
  
ggplot(numeric_long, aes(x = Feature, y = Value)) +
  geom_boxplot(outlier.colour = "red", outlier.alpha = 0.6) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Boxplot of Features (Before Outlier Removal)", y = "Value", x = "Feature")

#This is required because, PCA is based on variance and covariance
#Outliers inflate variance, distorting the true structure of the data.
#PCA may orient principal components toward those outliers.
#This results in misleading directions (principal axes) and poor downstream clustering (e.g., with K-Means).
#With outliers → PCs skewed toward a few distant points.
#Without outliers → PCs more representative of the data cloud

remove_outliers <- function(df) {
  df_clean <- df
  for (col in names(df_clean)) {
    Q1 <- quantile(df_clean[[col]], 0.25, na.rm = TRUE)
    Q3 <- quantile(df_clean[[col]], 0.75, na.rm = TRUE)
    IQR_val <- Q3 - Q1
    lower <- Q1 - 1.5 * IQR_val
    upper <- Q3 + 1.5 * IQR_val
    df_clean <- df_clean %>% filter(df_clean[[col]] >= lower & df_clean[[col]] <= upper)
  }
  return(df_clean)
}

numeric_features <- vehicles_clean %>% select(-class)
features_no_outliers <- remove_outliers(numeric_features)

# Add row ID for reshaping
features_no_outliers_long <- features_no_outliers %>%
  mutate(row_id = row_number()) %>%
  pivot_longer(-row_id, names_to = "Feature", values_to = "Value")

# Plot boxplots after outlier removal
ggplot(features_no_outliers_long, aes(x = Feature, y = Value)) +
  geom_boxplot(fill = "skyblue", outlier.shape = NA) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Boxplot of Features (After Outlier Removal)",
       x = "Feature", y = "Value")

print(nrow(features_no_outliers))

# Add class column back to the dataset
vehicles_no_outliers <- vehicles_clean %>%
  filter(rownames(.) %in% rownames(features_no_outliers))
  
print(vehicles_no_outliers)

# Scaling
# Applies z-score scaling to all numeric features
# Excludes the class column (since it's categorical)
# After this, all features are on the same scale, and now:
# - PCA won't be biased
# - K-Means won’t treat large-scale features as more “important”
features_scaled <- scale(vehicles_no_outliers %>% select(-class))
labels <- vehicles_no_outliers$class

# PCA Dimensionality Reduction
#center = TRUE: subtracts the mean of each feature before PCA.
#scale. = TRUE: divides by standard deviation (optional since scaled, but still ensures standardization).
pca_result <- prcomp(features_scaled, center = TRUE, scale. = TRUE)
summary(pca_result)
print(pca_result$rotation)

# Scree plot
fviz_eig(pca_result, addlabels = TRUE, ylim = c(0, 50))

# Cumulative variance 
cum_var <- cumsum(pca_result$sdev^2) / sum(pca_result$sdev^2)
cum_var_df <- data.frame(PC = 1:length(cum_var), CumulativeVariance = cum_var)

ggplot(cum_var_df, aes(x = PC, y = CumulativeVariance)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = 0.90, linetype = "dashed", color = "red") +
  labs(title = "Cumulative Variance Explained by Principal Components",
       y = "Cumulative Variance", x = "Number of Principal Components") +
  theme_minimal()
  
#The red dashed line is at 0.90 cumulative variance.
#The black curve crosses 0.90 somewhere between PC 4 and PC 5.
#Means:
#PC1–PC4 < 90%
#PC1–PC5 ≥ 90%
  
pca_selected <- pca_result$x[, 1:5]

#  visualization of first 2 PCs
pca_data <- as.data.frame(pca_result$x)
pca_data$class <- labels

ggplot(pca_data, aes(x = PC1, y = PC2, color = class)) +
  geom_point(alpha = 0.7) +
  labs(title = "PCA of Vehicle Features", x = "PC1", y = "PC2") +
  theme_minimal()

# Determine optimal number of clusters using Elbow Method
fviz_nbclust(pca_selected, kmeans, method = "wss")

# Apply K-Means
set.seed(123)
kmeans_result <- kmeans(pca_selected, centers = 4, nstart = 25)

# Add cluster info to PCA data for plotting
pca_data$cluster <- as.factor(kmeans_result$cluster)

# Visualize Clusters in PCA space
ggplot(pca_data, aes(x = PC1, y = PC2, color = cluster, shape = class)) +
  geom_point(alpha = 0.7) +
  labs(title = "K-Means Clustering on PCA-Reduced Data", x = "PC1", y = "PC2") +
  theme_minimal()

# Cluster Profiling
vehicles_no_outliers$cluster <- kmeans_result$cluster
cluster_means <- vehicles_no_outliers %>%
  group_by(cluster) %>%
  summarise(across(where(is.numeric), mean))
print(cluster_means, width = Inf)

# Compare Clusters with Actual Classes
print(table(Cluster = vehicles_no_outliers$cluster, Class = vehicles_no_outliers$class))

############################################## Hierarchical Clustering after PCA (R) ###############################################


# Compute distance
# Euclidean distance reflects how far two points are from each other in terms of overall variance-base and data is continous
distance_matrix <- dist(pca_selected, method = "euclidean")

#Agglomerative Clustering
# Ward method works only with squared Euclidean distances
hc <- hclust(distance_matrix, method = "ward.D2")

#Plot the Dendrogram
plot(hc, labels = FALSE, hang = -1, main = "Hierarchical Clustering Dendrogram")

#Cut Dendrogram into k Clusters
cluster_cut <- cutree(hc, k = 4)

# Plot PCA
pca_df <- as.data.frame(pca_selected)
pca_df$cluster <- factor(cluster_cut)
pca_df$class <- vehicles_no_outliers$class

ggplot(pca_df, aes(x = PC1, y = PC2, color = cluster, shape = class)) +
  geom_point(alpha = 0.7) +
  labs(title = "Hierarchical Clustering (Ward) on PCA-Reduced Data",
       x = "PC1", y = "PC2") +
  theme_minimal()
  
# Confusion Matrix
table(Cluster = pca_df$cluster, Class = pca_df$class)

